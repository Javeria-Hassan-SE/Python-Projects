{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed406c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import gradio as gr\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pyttsx3\n",
    "\n",
    "import winsound\n",
    "from scipy.io.wavfile import read, write\n",
    "from IPython.display import Audio\n",
    "from gtts import gTTS \n",
    "import os\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe7ac9",
   "metadata": {},
   "source": [
    "# Image Scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a553b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Use Gaussian Blurring combined with Adaptive Threshold** \n",
    "\n",
    "def blur_and_threshold(gray):\n",
    "    gray = cv2.GaussianBlur(gray,(3,3),2)\n",
    "    threshold = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n",
    "    threshold = cv2.fastNlMeansDenoising(threshold, 11, 31, 9)\n",
    "    return threshold\n",
    "\n",
    "\n",
    "# **Find the Biggest Contour** \n",
    "def biggest_contour(contours,min_area):\n",
    "    biggest = None\n",
    "    max_area = 0\n",
    "    biggest_n=0\n",
    "    approx_contour=None\n",
    "    for n,i in enumerate(contours):\n",
    "            area = cv2.contourArea(i)\n",
    "         \n",
    "            \n",
    "            if area > min_area/10:\n",
    "                    peri = cv2.arcLength(i,True)\n",
    "                    approx = cv2.approxPolyDP(i,0.02*peri,True)\n",
    "                    if area > max_area and len(approx)==4:\n",
    "                            biggest = approx\n",
    "                            max_area = area\n",
    "                            biggest_n=n\n",
    "                            approx_contour=approx\n",
    "                            \n",
    "                                                   \n",
    "    return biggest_n,approx_contour\n",
    "\n",
    "\n",
    "def order_points(pts):\n",
    "    # initialzie a list of coordinates that will be ordered\n",
    "    # such that the first entry in the list is the top-left,\n",
    "    # the second entry is the top-right, the third is the\n",
    "    # bottom-right, and the fourth is the bottom-left\n",
    "    pts=pts.reshape(4,2)\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "    # the top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # now, compute the difference between the points, the\n",
    "    # top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left will have the largest difference\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    # return the ordered coordinates\n",
    "    return rect\n",
    "\n",
    "\n",
    "# Find the exact (x,y) coordinates of the biggest contour and crop it out\n",
    "\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "    # obtain a consistent order of the points and unpack them\n",
    "    # individually\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "\n",
    "    # compute the width of the new image, which will be the\n",
    "    # maximum distance between bottom-right and bottom-left\n",
    "    # x-coordiates or the top-right and top-left x-coordinates\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "   \n",
    "\n",
    "    # compute the height of the new image, which will be the\n",
    "    # maximum distance between the top-right and bottom-right\n",
    "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    # now that we have the dimensions of the new image, construct\n",
    "    # the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, again specifying points\n",
    "    # in the top-left, top-right, bottom-right, and bottom-left\n",
    "    # order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # return the warped image\n",
    "    return warped\n",
    "\n",
    "\n",
    "# # Transformation the image\n",
    "\n",
    "# **1. Convert the image to grayscale**\n",
    "\n",
    "# **2. Remove noise and smoothen out the image by applying blurring and thresholding techniques**\n",
    "\n",
    "# **3. Use Canny Edge Detection to find the edges**\n",
    "\n",
    "# **4. Find the biggest contour and crop it out**\n",
    "\n",
    "\n",
    "def transformation(image):\n",
    "    image=image.copy()  \n",
    "    height, width, channels = image.shape\n",
    "    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image_size=gray.size\n",
    "    threshold=blur_and_threshold(gray)\n",
    "    edges = cv2.Canny(threshold,50,150,apertureSize = 7)\n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    simplified_contours = []\n",
    "    for cnt in contours:\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        simplified_contours.append(cv2.approxPolyDP(hull,\n",
    "                                0.001*cv2.arcLength(hull,True),True))\n",
    "    simplified_contours = np.array(simplified_contours)\n",
    "    biggest_n,approx_contour = biggest_contour(simplified_contours,image_size)\n",
    "\n",
    "    threshold = cv2.drawContours(image, simplified_contours ,biggest_n, (0,255,0), 1)\n",
    "\n",
    "    dst = 0\n",
    "    if approx_contour is not None and len(approx_contour)==4:\n",
    "        approx_contour=np.float32(approx_contour)\n",
    "        dst=four_point_transform(threshold,approx_contour)\n",
    "    else:\n",
    "        print(\"no contour found\")\n",
    "    croppedImage = dst\n",
    "    return croppedImage\n",
    "\n",
    "\n",
    "# **Increase the brightness of the image by playing with the \"V\" value (from HSV)**\n",
    "\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "    \n",
    "    #random_br = np.random.uniform(0.5,2.0)\n",
    "   # mask = hsv[:,:,2] * random_br > 255\n",
    "   # v = np.where(mask, 255, hsv[:,:,2] * random_br)\n",
    "   # hsv[:,:,2] = v\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img  \n",
    "    \n",
    "\n",
    "\n",
    "# **Sharpen the image using Kernel Sharpening Technique**\n",
    "\n",
    "\n",
    "def final_image(rotated):\n",
    "    kernel_sharpening = np.array([[0,-1,0], \n",
    "                                [-1, 5,-1],\n",
    "                                [0,-1,0]])\n",
    "    sharpened = cv2.filter2D(rotated, -1, kernel_sharpening)\n",
    "    sharpened=increase_brightness(sharpened,30)  \n",
    "    return sharpened\n",
    "\n",
    "def imageScanner(image):\n",
    "    blurred_threshold = transformation(image)\n",
    "    cleaned_image = final_image(blurred_threshold)\n",
    "    Final_Image=cv2.imwrite(\"gradio_output.png\", cleaned_image)\n",
    "    return cleaned_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff64b6",
   "metadata": {},
   "source": [
    "# Image to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1aa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text(image):\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    text=pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1b0be",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06029d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(image,language):\n",
    "    image_text=image_to_text(image)\n",
    "    if image_text!= None:\n",
    "        translator = Translator()\n",
    "        translated_text = translator.translate(image_text, src='en', dest = language)\n",
    "        speech = gTTS(translated_text.text, lang = language, slow = False)\n",
    "        file=speech.save(language+\".mp3\")\n",
    "        #os.system(\"start \"+language+\".mp3\")\n",
    "        return file \n",
    "    \n",
    "def play(image):\n",
    "    lang='en'\n",
    "    text_to_speech(image, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11670f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processor(image, options):\n",
    "    if options==\"Scan\":\n",
    "        return imageScanner(image)\n",
    "    if options==\"Text Extraction\":\n",
    "        return image_to_text(image)\n",
    "    if options==\"Convert to Audio\":\n",
    "        return play(image)\n",
    "    \n",
    "    \n",
    "def input_function(image, options):\n",
    "    prediction=image_processor(image, options)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b336df",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab29d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ff50355f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-19cccbc896be>:125: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  simplified_contours = np.array(simplified_contours)\n",
      "[2021-08-12 08:54:22,922] ERROR in app: Exception on /api/predict/ [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask_cors\\extension.py\", line 165, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\networking.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\networking.py\", line 179, in predict\n",
      "    prediction, durations = app.interface.process(raw_input)\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 329, in process\n",
      "    processed_output = [output_component.postprocess(\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 329, in <listcomp>\n",
      "    processed_output = [output_component.postprocess(\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\outputs.py\", line 339, in postprocess\n",
      "    return processing_utils.encode_file_to_base64(y, type=\"audio\", ext=\"wav\")\n",
      "  File \"C:\\Users\\Lab1\\anaconda3\\lib\\site-packages\\gradio\\processing_utils.py\", line 20, in encode_file_to_base64\n",
      "    with open(f, \"rb\") as file:\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 1: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "  fn=input_function, \n",
    "  inputs=[\"image\",gr.inputs.Radio([\"Scan\", \"Text Extraction\", \"Convert to Audio\"])],\n",
    "  outputs=[\"image\",\"text\",gr.outputs.Audio(label=\"Output Audio\", type=\"file\")],\n",
    "  title=\"Image Editor\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268dc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
